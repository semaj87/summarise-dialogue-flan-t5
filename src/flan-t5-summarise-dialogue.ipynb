{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68814cd",
   "metadata": {},
   "source": [
    "# Generative AI Task: Summarise Dialogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e74af3",
   "metadata": {},
   "source": [
    "This notebook has been used to perform the task of dialogue summarisation using Generative AI. Through the use of different techniques to the inference process, the exploration of how different prompts (input text) affect the completion (output) of the model, was performed. Prompt engineering was carried out, by comparing zero shot, one shot and few shot inferences, with the intention to see how to best enhance the generative output of the Large Language Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe60d2d",
   "metadata": {},
   "source": [
    "### Install the required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61e564",
   "metadata": {},
   "source": [
    "Given the scope of the task, we need to install packages to use PyTorch and Hugging Face transformers and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5062066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://james.aymer:****@nexus.tools.btcsp.co.uk/repository/pypi-group/simple\n",
      "Requirement already satisfied: torch==1.13.1 in ./venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from torch==1.13.1) (4.8.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://james.aymer:****@nexus.tools.btcsp.co.uk/repository/pypi-group/simple\n",
      "Requirement already satisfied: transformers==4.27.2 in ./venv/lib/python3.10/site-packages (4.27.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (23.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (4.66.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (2.31.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (0.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers==4.27.2) (1.26.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.2) (2023.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.27.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.27.2) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.27.2) (3.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13.1\n",
    "!pip install torchdata==0.5.1 --quiet\n",
    "!pip install transformers==4.27.2\n",
    "!pip install datasets==2.11.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b171f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3144f50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://james.aymer:****@nexus.tools.btcsp.co.uk/repository/pypi-group/simple\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (2.14.7)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.10/site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f57147",
   "metadata": {},
   "source": [
    "We need to load a few resources that will be used: datasets, Large Language Model (LLM), tokeniser and configurator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e8416",
   "metadata": {},
   "source": [
    "### Summarise dialogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1b822",
   "metadata": {},
   "source": [
    "In this use case, a summary of the dialogue will be generated with the pre-trained Large Language Model (LLM) `FLAN-T5` from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "We can now upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains over 10,000 dialogues with the corresponding manually labelled summaries and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ade14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (file:///Users/jamesaymer/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m huggingface_dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknkarthick/dialogsum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuggingface_dataset_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Jupyter/LLMs/FLAN-T5-dialogue-summary/venv/lib/python3.10/site-packages/datasets/load.py:1804\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1802\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1803\u001b[0m )\n\u001b[0;32m-> 1804\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;66;03m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Jupyter/LLMs/FLAN-T5-dialogue-summary/venv/lib/python3.10/site-packages/datasets/builder.py:1108\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1106\u001b[0m is_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local:\n\u001b[0;32m-> 1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a dataset cached in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1111\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: could not find data in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please make sure to call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilder.download_and_prepare(), or use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f423a2",
   "metadata": {},
   "source": [
    "Now that we have loaded the dataset, we can print a few dialogues from the dataset, together with their baseline summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fae6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Example: 1\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "INPUT DIALOGUE:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(dashed_line_ouptut_divider)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINPUT DIALOGUE:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(dashed_line_ouptut_divider)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBASELINE HUMAN SUMMARY:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# example_dialogue_indice_slice: list = [40, 200]\n",
    "\n",
    "dashed_line_ouptut_divider = '-'.join('' for x in range(116))\n",
    "\n",
    "for example, index in enumerate(example_dialogue_indice_slice):\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(f\"Example: {example + 1}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(\"INPUT DIALOGUE:\")\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(\"BASELINE HUMAN SUMMARY:\")\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959db68",
   "metadata": {},
   "source": [
    "Now we can load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), and create an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3710080",
   "metadata": {},
   "source": [
    "Before we can perform endcoding and decoding, we need to tokenize the text. `Tokenisation` is the process of splitting the texts into smaller units that can be processed by LLM models. This means that converting each word into a number representing a position in a dictionary of all the possible words that the model can work with.\n",
    "\n",
    "We can download the tokenizer for the `FLAN-T5` model using `AutoTOkenizer.from_pretrained()` method. The `use_fast` parameter can be used to use a fast Rust-based tokenizer if supported, otherwise a normal Python-based tokeniser is returned instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ada98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2baad",
   "metadata": {},
   "source": [
    "We can now test that the tokenizer can encode and decode a simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "# np = Numpy\n",
    "# tf = TensorFlow\n",
    "# pt = PyTorch\n",
    "\n",
    "encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "decoded_sentence = tokenizer.decode(\n",
    "        encoded_sentence[\"input_ids\"][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "print(\"ENCODED SENTENCE:\")\n",
    "print(encoded_sentence[\"input_ids\"][0])\n",
    "print(\"\\nDECODED SENTENCE:\")\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a29411",
   "metadata": {},
   "source": [
    "Now we can assess how well the base LLM summarises a dialogue without any prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, index in enumerate(example_dialogue_indice_slice):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(f\"Example: {example + 1}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "          \n",
    "    print(f\"INPUT PROMPT:\\n{dialogue}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "          \n",
    "    print(f\"MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf7675",
   "metadata": {},
   "source": [
    "It is clear that the guesses made by the model make some sense, but it doesn't seem exactly right. We can continue to use Prompt engineering to help here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5fd9c",
   "metadata": {},
   "source": [
    "### Zero shot inference with an instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, index in enumerate(example_dialogue_indice_slice):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(\"Example: {example + 1}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(f\"MODEL GENERATION - ZERO SHOT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2334f",
   "metadata": {},
   "source": [
    "### Zero shot inference with a Prompt template from FLAN-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2903db",
   "metadata": {},
   "source": [
    "We can use a [pre-built prompt template](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py) from `FLAN-T5` to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, index in enumerate(example_dialogue_indice_slice):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(\"Example: {example + 1}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\")\n",
    "    print(dashed_line_ouptut_divider)\n",
    "    \n",
    "    print(f\"MODEL GENERATION - ZERO SHOT:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830dbeb2",
   "metadata": {},
   "source": [
    "### One shot inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd0b17",
   "metadata": {},
   "source": [
    "We can build a function that takes a list of `example_indices_full`, generates a prompt with full exapmples, then at the end, it will append the prompt which you want the model to complete (`example_index_to_summarize`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = \"\"\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5...\n",
    "        # ...other models may have their own preferred stop sequence\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1bc0c6",
   "metadata": {},
   "source": [
    "We can now construct the prompt to perform one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a628c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc743ee",
   "metadata": {},
   "source": [
    "Now we can pass the prompt above to perform the one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90460b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\\n\")\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"MODEL GENERATION - ONE SHOT:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406aba9",
   "metadata": {},
   "source": [
    "### Few shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc2a86",
   "metadata": {},
   "source": [
    "We can add two more dialogue-summary pairs to the prompt, before performing few shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de882561",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f742277",
   "metadata": {},
   "source": [
    "As in the case of one shot inference, we can now pass the prompt to perform few shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\\n\")\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"MODEL GENERATION - FEW SHOT:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0a335",
   "metadata": {},
   "source": [
    "Given the output from the above cell, we can see that few shot inference did not provide much of an improvement over one shot inference. It is also important to remember to not exceed the model's input context length (`512 tokens`), as anything above the context length will be ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf214ce7",
   "metadata": {},
   "source": [
    "However, it is clear that passing in one full example (`one shot inference`), provides the model with more information to help improve the overall completion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb4dfa",
   "metadata": {},
   "source": [
    "### Changing Generative configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac6bf9",
   "metadata": {},
   "source": [
    "We can change some of the configuration parameters, to influence the way that the model makes the final decision about next word generation. We can change parameters such as `do_sample`, `temperature`, `top_k` & `top_p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"MODEL GENERATION - FEW SHOT:\\n{output}\")\n",
    "print(dashed_line_ouptut_divider)\n",
    "print(f\"BASELINE HUMAN SUMMARY:\\n{summary}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flan-t5-dialogue-summary",
   "language": "python",
   "name": "flan-t5-dialogue-summary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
